## ã€Šç¬¬ 23 ç«  è½»é‡çº§ç½‘ç»œæµ…æã€‹ å®ç°æºç 

<sup>docs</sup> <sup>code</sup> <sup>notebook</sup> <sup>model</sup>

## Code ğŸ‘†


<p align="left">
  <a href="https://github.com/Charmve/computer-vision-in-action"><img src="https://github.com/Charmve/computer-vision-in-action/blob/main/res/ui/workswith1.png?raw=true" title="Works with L0CV" width="120"></a>
</p>
<br>

## OpenChallenge

### AccANN
A accelerator based-on FPGA hardware with RTL (Verilog) for AdderNet, which is an adder networks (AdderNets) to trade massive multiplications in deep neural networks, especially convolutional neural networks (CNNs), for much cheaper additions to reduce computation costs.

<div align=center><img src="https://github.com/Charmve/AccANN/raw/main/img/figure/figure1.png"></div>

Fig 1. Visualization of features in AdderNets and CNNs. <sup>[1]</sup>


## arXiv


[1] AdderNet: Do We Really Need Multiplications in Deep Learning? Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, Chang Xu. CVPR, 2020. [ğŸ“‘[paper](https://arxiv.org/abs/1912.13200) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/huawei-noah/AdderNet)]

[2] AdderSR: Towards Energy Efficient Image Super-Resolution. Dehua Song, Yunhe Wang, Hanting Chen, Chang Xu, Chunjing Xu, Dacheng Tao. Arxiv, 2020. [ğŸ“‘[paper](https://arxiv.org/abs/2009.08891) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>code]

[3] ShiftAddNet: A Hardware-Inspired Deep Network. Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu, Zhangyang Wang, Yingyan Lin. NeurIPS, 2020. [ğŸ“‘[paper](https://arxiv.org/abs/2010.12785) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/RICE-EIC/ShiftAddNet)]

[4] Kernel Based Progressive Distillation for Adder Neural Networks. Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing XU, Yunhe Wang. NeurIPS, 2020. [ğŸ“‘[paper](https://arxiv.org/abs/2009.13044) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code]()]

[5] GhostNet: More Features from Cheap Operations [ğŸ“‘[paper](https://arxiv.org/abs/1911.11907) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/huawei-noah/ghostnet)]

[6] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications [ğŸ“‘[paper](https://arxiv.org/abs/1704.04861) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/Zehaos/MobileNet)]

[7] VarGNet: Variable Group Convolutional Neural Network for Efficient Embedded Computing. Qian Zhang, Jianjun Li, Meng Yao. [ğŸ“‘[paper](https://arxiv.org/pdf/1907.05653v1.pdf) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/zma-c-137/VarGFaceNet)]

[8] And the bit goes down: Revisiting the quantization of neural networks (ICLR 2020). Pierre Stock, Armand Joulin, Remi Gribonval. [ğŸ“‘[paper](https://arxiv.org/pdf/1907.05686.pdf) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/facebookresearch/kill-the-bits)]

[9] DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs [ğŸ“‘[paper](https://docs.wixstatic.com/ugd/c50250_77e06b7f02b44eacb76c05e8fbe01e08.pdf) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/IBM/AccDNN)]

[10] AdderNet and its Minimalist Hardware Design for Energy-Efficient Artificial Intelligence. Yunhe Wang, Mingqiang Huang, Kai Han, et.al. [ğŸ“‘[paper](https://arxiv.org/pdf/2101.10015.pdf) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/> code]

[11] PipeCNN: An OpenCL-Based Open-Source FPGA Accelerator for Convolution Neural Networks. FPT 2017. Dong Wang, Ke Xu and Diankun Jiang. [ğŸ“‘[paper](https://arxiv.org/abs/1611.02450) | <img src="https://img.icons8.com/material-sharp/24/000000/github.png" alt="Github" width="22px"/>[code](https://github.com/doonny/PipeCNN)]